# üß™ Sandbox de Testes para IA - GLPI Dashboard

## üéØ Objetivo

O sandbox fornece um ambiente seguro e isolado para testar integra√ß√µes de IA, prompts autom√°ticos e funcionalidades experimentais sem afetar o ambiente de produ√ß√£o.

## üèóÔ∏è Estrutura do Sandbox

```
docs/ai/sandbox/
‚îú‚îÄ‚îÄ README.md                 # Este arquivo
‚îú‚îÄ‚îÄ test-prompts/            # Prompts para teste
‚îÇ   ‚îú‚îÄ‚îÄ feature-development.md
‚îÇ   ‚îú‚îÄ‚îÄ bug-analysis.md
‚îÇ   ‚îî‚îÄ‚îÄ code-review.md
‚îú‚îÄ‚îÄ mock-data/               # Dados simulados
‚îÇ   ‚îú‚îÄ‚îÄ glpi-responses/
‚îÇ   ‚îú‚îÄ‚îÄ api-contracts/
‚îÇ   ‚îî‚îÄ‚îÄ test-scenarios/
‚îú‚îÄ‚îÄ experiments/             # Experimentos de IA
‚îÇ   ‚îú‚îÄ‚îÄ prompt-optimization/
‚îÇ   ‚îú‚îÄ‚îÄ code-generation/
‚îÇ   ‚îî‚îÄ‚îÄ documentation-auto/
‚îî‚îÄ‚îÄ results/                 # Resultados dos testes
    ‚îú‚îÄ‚îÄ performance/
    ‚îú‚îÄ‚îÄ accuracy/
    ‚îî‚îÄ‚îÄ reports/
```

## üöÄ Como Usar o Sandbox

### 1. Configura√ß√£o Inicial

```bash
# Navegar para o diret√≥rio do projeto
cd glpi_dashboard

# Criar ambiente virtual para sandbox (opcional)
python -m venv sandbox-env
source sandbox-env/bin/activate  # Linux/Mac
# ou
sandbox-env\Scripts\activate     # Windows

# Instalar depend√™ncias de desenvolvimento
pip install -r requirements-dev.txt
```

### 2. Executar Testes de Prompt

```bash
# Testar prompts de desenvolvimento
python docs/ai/sandbox/scripts/test_prompts.py --category development

# Testar prompts de debug
python docs/ai/sandbox/scripts/test_prompts.py --category debug

# Executar todos os testes
python docs/ai/sandbox/scripts/run_all_tests.py
```

### 3. Simular Integra√ß√µes

```bash
# Simular chamadas GLPI com dados mock
python docs/ai/sandbox/scripts/simulate_glpi.py

# Testar gera√ß√£o autom√°tica de c√≥digo
python docs/ai/sandbox/scripts/test_code_generation.py

# Validar documenta√ß√£o autom√°tica
python docs/ai/sandbox/scripts/test_auto_docs.py
```

## üß™ Tipos de Teste Dispon√≠veis

### 1. Testes de Prompt

**Objetivo**: Validar efic√°cia dos prompts contextuais

**M√©tricas**:
- Precis√£o das respostas
- Tempo de resposta
- Qualidade do c√≥digo gerado
- Ader√™ncia aos padr√µes do projeto

**Exemplo**:
```python
# docs/ai/sandbox/scripts/test_prompts.py
import json
from typing import Dict, List

class PromptTester:
    def test_development_prompt(self, scenario: str) -> Dict:
        """Testa prompt de desenvolvimento com cen√°rio espec√≠fico"""
        prompt = self.load_prompt('development', scenario)
        response = self.simulate_ai_response(prompt)
        
        return {
            'accuracy': self.measure_accuracy(response),
            'completeness': self.check_completeness(response),
            'code_quality': self.analyze_code_quality(response),
            'adherence': self.check_standards_adherence(response)
        }
```

### 2. Simula√ß√£o de Dados

**Objetivo**: Testar com dados realistas sem afetar sistemas reais

**Dados Dispon√≠veis**:
- Respostas GLPI simuladas
- Cen√°rios de erro
- Dados de performance
- Casos extremos

**Exemplo**:
```json
// docs/ai/sandbox/mock-data/glpi-responses/tickets.json
{
  "data": [
    {
      "id": "12345",
      "name": "Problema de rede",
      "status": "2",
      "priority": "3",
      "date_creation": "2024-01-15 10:30:00",
      "date_mod": "2024-01-15 14:20:00"
    }
  ],
  "totalcount": 150,
  "count": 50
}
```

### 3. Testes de Performance

**Objetivo**: Medir impacto da IA na performance do sistema

**M√©tricas**:
- Tempo de processamento
- Uso de mem√≥ria
- Throughput
- Lat√™ncia

**Exemplo**:
```python
# docs/ai/sandbox/scripts/performance_test.py
import time
import psutil
from typing import Dict

class PerformanceTester:
    def measure_ai_impact(self, operation: str) -> Dict:
        """Mede impacto da IA em opera√ß√µes espec√≠ficas"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        # Executar opera√ß√£o com IA
        result = self.execute_with_ai(operation)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        
        return {
            'execution_time': end_time - start_time,
            'memory_usage': end_memory - start_memory,
            'result_quality': self.assess_quality(result)
        }
```

## üìä Cen√°rios de Teste

### Cen√°rio 1: Desenvolvimento de Nova Feature

**Descri√ß√£o**: Simular desenvolvimento de endpoint para relat√≥rios

**Entrada**:
```markdown
Criar endpoint /api/reports/summary que retorna:
- Total de tickets por status
- Tempo m√©dio de resolu√ß√£o
- Top 5 categorias mais frequentes
- Dados dos √∫ltimos 30 dias
```

**Sa√≠da Esperada**:
- C√≥digo Python funcional
- Testes unit√°rios
- Documenta√ß√£o da API
- Integra√ß√£o com frontend

**Valida√ß√£o**:
- C√≥digo executa sem erros
- Testes passam
- Segue padr√µes do projeto
- Performance adequada

### Cen√°rio 2: Debug de Issue

**Descri√ß√£o**: Diagnosticar problema de performance

**Entrada**:
```markdown
API /api/metrics est√° lenta (>2s resposta)
Logs mostram:
- M√∫ltiplas chamadas GLPI
- Cache miss frequente
- Timeout ocasional
```

**Sa√≠da Esperada**:
- An√°lise das causas
- Plano de corre√ß√£o
- C√≥digo otimizado
- Testes de regress√£o

### Cen√°rio 3: Refatora√ß√£o de C√≥digo

**Descri√ß√£o**: Melhorar estrutura do GLPIService

**Entrada**:
```python
# C√≥digo atual com problemas
class GLPIService:
    def get_data(self, type, filters):
        # C√≥digo monol√≠tico
        # Sem tratamento de erro
        # Sem cache
        pass
```

**Sa√≠da Esperada**:
- C√≥digo refatorado
- Separa√ß√£o de responsabilidades
- Tratamento de erros
- Cache implementado
- Testes atualizados

## üîß Scripts de Automa√ß√£o

### Script Principal de Testes

```python
#!/usr/bin/env python3
# docs/ai/sandbox/scripts/run_sandbox.py

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List

class SandboxRunner:
    def __init__(self, config_path: str = None):
        self.config = self.load_config(config_path)
        self.results = []
    
    def run_all_tests(self) -> Dict:
        """Executa todos os testes do sandbox"""
        results = {
            'prompt_tests': self.run_prompt_tests(),
            'performance_tests': self.run_performance_tests(),
            'integration_tests': self.run_integration_tests(),
            'code_quality_tests': self.run_code_quality_tests()
        }
        
        self.generate_report(results)
        return results
    
    def run_prompt_tests(self) -> List[Dict]:
        """Executa testes de prompts"""
        test_cases = self.load_test_cases('prompts')
        results = []
        
        for case in test_cases:
            result = self.execute_prompt_test(case)
            results.append(result)
            
        return results
    
    def generate_report(self, results: Dict) -> None:
        """Gera relat√≥rio dos testes"""
        report_path = Path('docs/ai/sandbox/results/latest_report.json')
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"Relat√≥rio gerado: {report_path}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Sandbox de Testes IA')
    parser.add_argument('--config', help='Arquivo de configura√ß√£o')
    parser.add_argument('--test-type', choices=['all', 'prompts', 'performance', 'integration'])
    
    args = parser.parse_args()
    
    runner = SandboxRunner(args.config)
    
    if args.test_type == 'all':
        results = runner.run_all_tests()
    elif args.test_type == 'prompts':
        results = runner.run_prompt_tests()
    # ... outros tipos
    
    print(f"Testes conclu√≠dos. Resultados: {len(results)} casos testados")
```

### Configura√ß√£o do Sandbox

```json
// docs/ai/sandbox/config/sandbox.json
{
  "environment": "sandbox",
  "ai_models": {
    "primary": "claude-3-sonnet",
    "fallback": "gpt-4"
  },
  "test_settings": {
    "timeout": 30,
    "max_retries": 3,
    "parallel_tests": 4
  },
  "mock_data": {
    "glpi_url": "http://localhost:8080/mock-glpi",
    "redis_url": "redis://localhost:6379/15"
  },
  "metrics": {
    "accuracy_threshold": 0.85,
    "performance_threshold": 2.0,
    "quality_threshold": 0.90
  }
}
```

## üìà M√©tricas e Relat√≥rios

### Dashboard de M√©tricas

```python
# docs/ai/sandbox/scripts/metrics_dashboard.py
import streamlit as st
import pandas as pd
import plotly.express as px

def create_dashboard():
    st.title('üß™ Sandbox IA - Dashboard de M√©tricas')
    
    # Carregar dados dos testes
    results = load_test_results()
    
    # M√©tricas principais
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric('Precis√£o M√©dia', f"{results['accuracy']:.2%}")
    
    with col2:
        st.metric('Tempo M√©dio', f"{results['avg_time']:.2f}s")
    
    with col3:
        st.metric('Qualidade C√≥digo', f"{results['code_quality']:.2%}")
    
    with col4:
        st.metric('Taxa Sucesso', f"{results['success_rate']:.2%}")
    
    # Gr√°ficos
    st.subheader('Performance por Tipo de Teste')
    fig = px.bar(results['by_type'], x='test_type', y='performance')
    st.plotly_chart(fig)
    
    # Hist√≥rico
    st.subheader('Hist√≥rico de Testes')
    fig_timeline = px.line(results['timeline'], x='date', y='accuracy')
    st.plotly_chart(fig_timeline)

if __name__ == '__main__':
    create_dashboard()
```

### Executar Dashboard

```bash
# Instalar depend√™ncias
pip install streamlit plotly

# Executar dashboard
streamlit run docs/ai/sandbox/scripts/metrics_dashboard.py
```

## üîí Seguran√ßa e Isolamento

### Medidas de Seguran√ßa

1. **Isolamento de Dados**:
   - Dados mock apenas
   - Sem acesso a produ√ß√£o
   - Ambiente containerizado

2. **Controle de Acesso**:
   - Tokens espec√≠ficos para sandbox
   - Rate limiting agressivo
   - Logs de todas as opera√ß√µes

3. **Valida√ß√£o de C√≥digo**:
   - An√°lise est√°tica autom√°tica
   - Verifica√ß√£o de vulnerabilidades
   - Sandbox de execu√ß√£o

### Docker para Isolamento

```dockerfile
# docs/ai/sandbox/Dockerfile
FROM python:3.11-slim

WORKDIR /sandbox

# Instalar depend√™ncias
COPY requirements-sandbox.txt .
RUN pip install -r requirements-sandbox.txt

# Copiar c√≥digo do sandbox
COPY docs/ai/sandbox/ .

# Usu√°rio n√£o-root
RUN useradd -m sandbox
USER sandbox

# Comando padr√£o
CMD ["python", "scripts/run_sandbox.py"]
```

```bash
# Construir e executar
docker build -t glpi-sandbox docs/ai/sandbox/
docker run --rm -v $(pwd)/docs/ai/sandbox/results:/sandbox/results glpi-sandbox
```

## üìö Documenta√ß√£o dos Resultados

### Formato de Relat√≥rio

```json
{
  "test_run": {
    "id": "run_20240115_143022",
    "timestamp": "2024-01-15T14:30:22Z",
    "duration": 45.2,
    "environment": "sandbox"
  },
  "summary": {
    "total_tests": 25,
    "passed": 23,
    "failed": 2,
    "success_rate": 0.92
  },
  "metrics": {
    "accuracy": 0.89,
    "performance": 1.8,
    "code_quality": 0.94,
    "adherence": 0.91
  },
  "details": [
    {
      "test_id": "prompt_dev_001",
      "category": "development",
      "status": "passed",
      "metrics": {
        "accuracy": 0.95,
        "time": 2.1,
        "quality": 0.92
      }
    }
  ],
  "recommendations": [
    "Otimizar prompt para cen√°rios de debug",
    "Adicionar mais exemplos de c√≥digo",
    "Melhorar valida√ß√£o de entrada"
  ]
}
```

## üöÄ Pr√≥ximos Passos

1. **Implementar Scripts Base**:
   - `run_sandbox.py`
   - `test_prompts.py`
   - `metrics_dashboard.py`

2. **Criar Dados Mock**:
   - Respostas GLPI realistas
   - Cen√°rios de erro
   - Casos extremos

3. **Configurar CI/CD**:
   - Testes autom√°ticos
   - Relat√≥rios cont√≠nuos
   - Alertas de regress√£o

4. **Expandir Cobertura**:
   - Mais tipos de teste
   - Integra√ß√£o com ferramentas externas
   - Testes de acessibilidade

---

**AI Context Tags**: `sandbox`, `testing`, `ai-integration`, `automation`, `performance`
**Related Files**: `docs/ai/prompts/development.md`, `backend/tests/`, `scripts/`
**Last Updated**: 2024-01-15