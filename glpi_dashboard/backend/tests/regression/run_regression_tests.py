#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de execu√ß√£o de testes de regress√£o para o GLPI Dashboard

Este script automatiza a execu√ß√£o de testes de regress√£o usando o
MetricsRegressionComparator para detectar diverg√™ncias nas m√©tricas.
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

# Adicionar diret√≥rio do comparador ao path
sys.path.insert(0, str(Path(__file__).parent))

from metrics_regression_comparator import MetricsRegressionComparator


def setup_baseline_snapshots():
    """Configura snapshots baseline para todos os endpoints principais"""
    print("üîß Configurando snapshots baseline...")

    comparator = MetricsRegressionComparator()

    # Endpoints principais para capturar
    endpoints = [
        {
            "endpoint": "/metrics",
            "name": "dashboard_metrics_baseline",
            "params": {},
            "description": "M√©tricas principais do dashboard",
        },
        {
            "endpoint": "/metrics",
            "name": "dashboard_metrics_with_dates",
            "params": {"start_date": "2024-01-01", "end_date": "2024-12-31"},
            "description": "M√©tricas com filtro de data",
        },
        {
            "endpoint": "/metrics/filtered",
            "name": "filtered_metrics_novo",
            "params": {"status": "novo"},
            "description": "M√©tricas filtradas por status novo",
        },
        {
            "endpoint": "/metrics/filtered",
            "name": "filtered_metrics_progresso",
            "params": {"status": "progresso"},
            "description": "M√©tricas filtradas por status em progresso",
        },
        {
            "endpoint": "/technicians/ranking",
            "name": "technician_ranking_default",
            "params": {"limit": 10},
            "description": "Ranking de t√©cnicos padr√£o",
        },
        {
            "endpoint": "/technicians/ranking",
            "name": "technician_ranking_n1",
            "params": {"limit": 10, "level": "N1"},
            "description": "Ranking de t√©cnicos N1",
        },
        {
            "endpoint": "/technicians/ranking",
            "name": "technician_ranking_n2",
            "params": {"limit": 10, "level": "N2"},
            "description": "Ranking de t√©cnicos N2",
        },
        {
            "endpoint": "/technicians/ranking",
            "name": "technician_ranking_n3",
            "params": {"limit": 10, "level": "N3"},
            "description": "Ranking de t√©cnicos N3",
        },
        {
            "endpoint": "/technicians/ranking",
            "name": "technician_ranking_n4",
            "params": {"limit": 10, "level": "N4"},
            "description": "Ranking de t√©cnicos N4",
        },
        {
            "endpoint": "/tickets/new",
            "name": "new_tickets_default",
            "params": {"limit": 5},
            "description": "Tickets novos padr√£o",
        },
    ]

    captured_snapshots = []

    for endpoint_config in endpoints:
        try:
            print(f"\nüì∏ Capturando: {endpoint_config['description']}")
            print(f"   Endpoint: {endpoint_config['endpoint']}")
            print(f"   Par√¢metros: {endpoint_config['params']}")

            snapshot_path = comparator.capture_baseline_snapshot(
                endpoint_config["endpoint"],
                endpoint_config["params"],
                f"{endpoint_config['name']}.json",
            )

            captured_snapshots.append(
                {
                    "name": endpoint_config["name"],
                    "path": snapshot_path,
                    "endpoint": endpoint_config["endpoint"],
                    "params": endpoint_config["params"],
                    "description": endpoint_config["description"],
                }
            )

            print(f"   ‚úÖ Capturado: {snapshot_path}")

        except Exception as e:
            print(f"   ‚ùå Erro: {e}")

    # Salvar √≠ndice de snapshots
    index_path = comparator.snapshots_dir / "snapshots_index.json"
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "created_at": datetime.now().isoformat(),
                "total_snapshots": len(captured_snapshots),
                "snapshots": captured_snapshots,
            },
            f,
            indent=2,
            ensure_ascii=False,
        )

    print(f"\n‚úÖ Configura√ß√£o conclu√≠da: {len(captured_snapshots)} snapshots capturados")
    print(f"üìÑ √çndice salvo: {index_path}")

    return captured_snapshots


def run_full_regression_suite():
    """Executa suite completa de testes de regress√£o"""
    print("üß™ Executando suite completa de testes de regress√£o...")

    comparator = MetricsRegressionComparator(tolerance=0.05)  # 5% de toler√¢ncia

    # Carregar √≠ndice de snapshots
    index_path = comparator.snapshots_dir / "snapshots_index.json"

    if not index_path.exists():
        print(
            "‚ùå √çndice de snapshots n√£o encontrado. Execute setup_baseline_snapshots() primeiro."
        )
        return False

    with open(index_path, "r", encoding="utf-8") as f:
        index_data = json.load(f)

    snapshots = index_data["snapshots"]

    print(f"üìã Encontrados {len(snapshots)} snapshots para testar")

    all_reports = []
    total_tests = len(snapshots)
    passed_tests = 0
    failed_tests = 0

    for i, snapshot_config in enumerate(snapshots, 1):
        print(f"\n[{i}/{total_tests}] üîç Testando: {snapshot_config['description']}")

        try:
            # Verificar se snapshot existe
            if not os.path.exists(snapshot_config["path"]):
                print(f"   ‚ùå Snapshot n√£o encontrado: {snapshot_config['path']}")
                failed_tests += 1
                continue

            # Executar teste de regress√£o
            report = comparator.run_regression_test(
                snapshot_config["path"],
                snapshot_config["endpoint"],
                snapshot_config["params"],
                snapshot_config["name"],
            )

            all_reports.append(report)

            # Mostrar resultado resumido
            if report.success:
                print(
                    f"   ‚úÖ PASSOU: {report.passed_comparisons}/{report.total_comparisons} compara√ß√µes"
                )
                passed_tests += 1
            else:
                print(
                    f"   ‚ùå FALHOU: {report.failed_comparisons}/{report.total_comparisons} diferen√ßas encontradas"
                )
                failed_tests += 1

                # Mostrar primeiras 3 diferen√ßas
                failed_diffs = [d for d in report.differences if not d.match][:3]
                for diff in failed_diffs:
                    print(
                        f"      ‚Ä¢ {diff.field_path}: {diff.expected_value} ‚Üí {diff.actual_value}"
                    )

                if len(failed_diffs) < report.failed_comparisons:
                    remaining = report.failed_comparisons - len(failed_diffs)
                    print(f"      ... e mais {remaining} diferen√ßas")

            print(f"   ‚è±Ô∏è  Tempo: {report.execution_time:.2f}s")

            # Salvar relat√≥rio individual
            comparator.save_report(report)

        except Exception as e:
            print(f"   ‚ùå ERRO: {e}")
            failed_tests += 1

    # Gerar relat√≥rio consolidado
    generate_consolidated_report(all_reports, comparator)

    # Resumo final
    print(f"\n{'='*60}")
    print(f"RESULTADO FINAL DA SUITE DE REGRESS√ÉO")
    print(f"{'='*60}")
    print(f"Total de testes: {total_tests}")
    print(f"Testes aprovados: {passed_tests}")
    print(f"Testes falharam: {failed_tests}")
    print(f"Taxa de sucesso: {(passed_tests/total_tests)*100:.1f}%")

    if failed_tests == 0:
        print(f"\nüéâ TODOS OS TESTES PASSARAM! Nenhuma regress√£o detectada.")
    else:
        print(f"\n‚ö†Ô∏è  {failed_tests} TESTES FALHARAM. Regress√µes detectadas!")

    print(f"{'='*60}")

    return failed_tests == 0


def generate_consolidated_report(reports, comparator):
    """Gera relat√≥rio consolidado de todos os testes"""
    timestamp = datetime.now()

    # Calcular estat√≠sticas gerais
    total_tests = len(reports)
    passed_tests = len([r for r in reports if r.success])
    failed_tests = total_tests - passed_tests
    total_comparisons = sum(r.total_comparisons for r in reports)
    total_differences = sum(r.failed_comparisons for r in reports)
    avg_execution_time = (
        sum(r.execution_time for r in reports) / total_tests if total_tests > 0 else 0
    )

    # Agrupar diferen√ßas por tipo
    differences_by_type = {}
    for report in reports:
        for diff in report.differences:
            if not diff.match:
                diff_type = diff.difference_type
                if diff_type not in differences_by_type:
                    differences_by_type[diff_type] = []
                differences_by_type[diff_type].append(
                    {
                        "test": report.test_name,
                        "field": diff.field_path,
                        "expected": diff.expected_value,
                        "actual": diff.actual_value,
                    }
                )

    # Criar relat√≥rio consolidado
    consolidated_report = {
        "metadata": {
            "timestamp": timestamp.isoformat(),
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": (passed_tests / total_tests) * 100
            if total_tests > 0
            else 0,
            "total_comparisons": total_comparisons,
            "total_differences": total_differences,
            "avg_execution_time": avg_execution_time,
        },
        "test_results": [
            {
                "test_name": report.test_name,
                "success": report.success,
                "total_comparisons": report.total_comparisons,
                "failed_comparisons": report.failed_comparisons,
                "execution_time": report.execution_time,
            }
            for report in reports
        ],
        "differences_by_type": differences_by_type,
        "failed_tests_details": [
            {
                "test_name": report.test_name,
                "failed_comparisons": report.failed_comparisons,
                "differences": [
                    {
                        "field_path": diff.field_path,
                        "expected_value": diff.expected_value,
                        "actual_value": diff.actual_value,
                        "difference_type": diff.difference_type,
                    }
                    for diff in report.differences
                    if not diff.match
                ],
            }
            for report in reports
            if not report.success
        ],
    }

    # Salvar relat√≥rio consolidado
    report_filename = (
        f"consolidated_regression_report_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
    )
    report_path = comparator.reports_dir / report_filename

    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(consolidated_report, f, indent=2, ensure_ascii=False)

    print(f"\nüìä Relat√≥rio consolidado salvo: {report_path}")

    return str(report_path)


def quick_smoke_test():
    """Executa teste r√°pido de smoke nos endpoints principais"""
    print("üí® Executando teste r√°pido de smoke...")

    comparator = MetricsRegressionComparator(
        tolerance=0.1
    )  # Toler√¢ncia maior para smoke test

    # Endpoints cr√≠ticos para smoke test
    critical_endpoints = [
        {"endpoint": "/metrics", "params": {}, "name": "smoke_dashboard_metrics"},
        {
            "endpoint": "/technicians/ranking",
            "params": {"limit": 5},
            "name": "smoke_technician_ranking",
        },
    ]

    all_passed = True

    for endpoint_config in critical_endpoints:
        print(f"\nüîç Testando: {endpoint_config['name']}")

        try:
            # Capturar snapshot tempor√°rio
            temp_snapshot = comparator.capture_baseline_snapshot(
                endpoint_config["endpoint"],
                endpoint_config["params"],
                f"temp_{endpoint_config['name']}.json",
            )

            # Aguardar um pouco e testar novamente (deve ser id√™ntico)
            import time

            time.sleep(1)

            report = comparator.run_regression_test(
                temp_snapshot,
                endpoint_config["endpoint"],
                endpoint_config["params"],
                endpoint_config["name"],
            )

            if report.success:
                print(f"   ‚úÖ PASSOU: Endpoint est√° est√°vel")
            else:
                print(
                    f"   ‚ùå FALHOU: Endpoint inst√°vel - {report.failed_comparisons} diferen√ßas"
                )
                all_passed = False

            # Limpar snapshot tempor√°rio
            os.remove(temp_snapshot)

        except Exception as e:
            print(f"   ‚ùå ERRO: {e}")
            all_passed = False

    result = "‚úÖ SMOKE TEST PASSOU" if all_passed else "‚ùå SMOKE TEST FALHOU"
    print(f"\n{result}")

    return all_passed


def main():
    """Fun√ß√£o principal"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Executar testes de regress√£o para GLPI Dashboard"
    )
    parser.add_argument(
        "--action",
        choices=["setup", "test", "smoke", "full"],
        default="full",
        help="A√ß√£o a executar",
    )

    args = parser.parse_args()

    print(f"üöÄ Iniciando testes de regress√£o - A√ß√£o: {args.action}")
    print(f"‚è∞ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        if args.action == "setup":
            setup_baseline_snapshots()

        elif args.action == "smoke":
            success = quick_smoke_test()
            sys.exit(0 if success else 1)

        elif args.action == "test" or args.action == "full":
            # Verificar se snapshots existem
            comparator = MetricsRegressionComparator()
            index_path = comparator.snapshots_dir / "snapshots_index.json"

            if not index_path.exists():
                print("‚ö†Ô∏è  Snapshots baseline n√£o encontrados. Configurando...")
                setup_baseline_snapshots()

            success = run_full_regression_suite()
            sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Testes interrompidos pelo usu√°rio")
        sys.exit(1)

    except Exception as e:
        print(f"\n‚ùå Erro durante execu√ß√£o: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
