#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Configura√ß√£o M√≠nima GPU - GLPI Dashboard
Instala√ß√£o otimizada para limita√ß√µes de espa√ßo em disco
"""

import os
import sys
import subprocess
import logging
from pathlib import Path
from datetime import datetime

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class GPUMinimalSetup:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent.parent
        self.backend_dir = self.project_root / "glpi_dashboard" / "backend"
        self.scripts_dir = self.backend_dir / "scripts"

        # Pacotes essenciais apenas
        self.essential_packages = [
            "torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
            "transformers",
            "accelerate",
            "datasets",
            "tokenizers"
        ]

        # Modelos compactos recomendados
        self.compact_models = [
            "microsoft/DialoGPT-small",  # 117MB
            "distilbert-base-uncased",   # 268MB
            "microsoft/CodeBERT-base"    # 501MB
        ]

    def check_gpu(self):
        """Verifica√ß√£o b√°sica de GPU"""
        logger.info("üîç Verificando GPU...")
        try:
            result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
            if result.returncode == 0:
                logger.info("‚úÖ NVIDIA GPU detectada")
                return True
            else:
                logger.warning("‚ö†Ô∏è GPU NVIDIA n√£o detectada")
                return False
        except FileNotFoundError:
            logger.warning("‚ö†Ô∏è nvidia-smi n√£o encontrado")
            return False

    def install_package(self, package):
        """Instala um pacote espec√≠fico"""
        logger.info(f"üì¶ Instalando: {package}")
        try:
            cmd = f"pip install {package}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

            if result.returncode == 0:
                logger.info(f"‚úÖ Sucesso: {package.split()[0]}")
                return True
            else:
                logger.error(f"‚ùå Falha: {package} - {result.stderr[:100]}...")
                return False
        except Exception as e:
            logger.error(f"‚ùå Erro ao instalar {package}: {str(e)}")
            return False

    def install_essentials(self):
        """Instala apenas pacotes essenciais"""
        logger.info("üì¶ Instalando pacotes essenciais...")
        success_count = 0

        for package in self.essential_packages:
            if self.install_package(package):
                success_count += 1

        logger.info(f"üìä Instalados: {success_count}/{len(self.essential_packages)} pacotes")
        return success_count > 0

    def test_pytorch_gpu(self):
        """Testa PyTorch com GPU"""
        logger.info("üß™ Testando PyTorch + GPU...")
        try:
            import torch

            # Informa√ß√µes b√°sicas
            logger.info(f"PyTorch vers√£o: {torch.__version__}")
            logger.info(f"CUDA dispon√≠vel: {torch.cuda.is_available()}")

            if torch.cuda.is_available():
                logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
                logger.info(f"CUDA vers√£o: {torch.version.cuda}")

                # Teste simples
                x = torch.randn(3, 3).cuda()
                y = torch.randn(3, 3).cuda()
                z = torch.mm(x, y)
                logger.info("‚úÖ Teste de opera√ß√£o GPU: OK")
                return True
            else:
                logger.warning("‚ö†Ô∏è CUDA n√£o dispon√≠vel no PyTorch")
                return False

        except ImportError:
            logger.error("‚ùå PyTorch n√£o instalado")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erro no teste: {str(e)}")
            return False

    def create_minimal_orchestrator(self):
        """Cria orquestrador m√≠nimo"""
        logger.info("ü§ñ Criando orquestrador m√≠nimo...")

        orchestrator_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Orquestrador M√≠nimo de IA - GLPI Dashboard
"""

import torch
from transformers import AutoTokenizer, AutoModel
import logging

class GLPIAIOrchestrator:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.models = {}
        self.tokenizers = {}

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

        self.logger.info(f"üöÄ Orquestrador iniciado - Dispositivo: {self.device}")

    def load_model(self, model_name, alias=None):
        """Carrega um modelo compacto"""
        try:
            alias = alias or model_name.split("/")[-1]

            self.logger.info(f"üì• Carregando modelo: {model_name}")

            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)

            if self.device == "cuda":
                model = model.to(self.device)

            self.tokenizers[alias] = tokenizer
            self.models[alias] = model

            self.logger.info(f"‚úÖ Modelo carregado: {alias}")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar {model_name}: {str(e)}")
            return False

    def get_embeddings(self, text, model_alias="distilbert-base-uncased"):
        """Gera embeddings de texto"""
        try:
            if model_alias not in self.models:
                self.logger.warning(f"Modelo {model_alias} n√£o carregado")
                return None

            tokenizer = self.tokenizers[model_alias]
            model = self.models[model_alias]

            inputs = tokenizer(text, return_tensors="pt", truncate=True, max_length=512)

            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)

            return embeddings.cpu().numpy()

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao gerar embeddings: {str(e)}")
            return None

    def analyze_glpi_log(self, log_text):
        """An√°lise b√°sica de logs GLPI"""
        try:
            # An√°lise simples baseada em palavras-chave
            keywords = {
                "error": ["error", "erro", "failed", "exception"],
                "warning": ["warning", "warn", "alerta"],
                "info": ["info", "success", "ok", "completed"]
            }

            analysis = {"type": "info", "confidence": 0.5, "keywords": []}

            log_lower = log_text.lower()

            for category, words in keywords.items():
                found_words = [word for word in words if word in log_lower]
                if found_words:
                    analysis["keywords"].extend(found_words)
                    if category == "error":
                        analysis["type"] = "error"
                        analysis["confidence"] = 0.8
                    elif category == "warning" and analysis["type"] != "error":
                        analysis["type"] = "warning"
                        analysis["confidence"] = 0.7

            return analysis

        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise: {str(e)}")
            return {"type": "unknown", "confidence": 0.0, "keywords": []}

    def get_status(self):
        """Status do orquestrador"""
        return {
            "device": self.device,
            "models_loaded": list(self.models.keys()),
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
        }

if __name__ == "__main__":
    # Teste b√°sico
    orchestrator = GLPIAIOrchestrator()
    print("Status:", orchestrator.get_status())

    # Teste de an√°lise
    test_log = "Error connecting to database: connection timeout"
    result = orchestrator.analyze_glpi_log(test_log)
    print("An√°lise:", result)
'''

        orchestrator_path = self.scripts_dir / "glpi_ai_orchestrator_minimal.py"
        orchestrator_path.write_text(orchestrator_code, encoding='utf-8')
        logger.info(f"ü§ñ Orquestrador criado: {orchestrator_path}")
        return orchestrator_path

    def create_test_script(self):
        """Cria script de teste m√≠nimo"""
        logger.info("üß™ Criando script de teste...")

        test_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Teste M√≠nimo - Integra√ß√£o GPU GLPI
"""

import sys
import torch
from pathlib import Path

# Adiciona o diret√≥rio de scripts ao path
sys.path.append(str(Path(__file__).parent))

def test_environment():
    """Testa ambiente b√°sico"""
    print("üß™ Teste de Ambiente GPU - GLPI Dashboard")
    print("=" * 50)

    # Teste PyTorch
    print(f"PyTorch vers√£o: {torch.__version__}")
    print(f"CUDA dispon√≠vel: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA vers√£o: {torch.version.cuda}")
        print(f"Mem√≥ria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

        # Teste simples
        try:
            x = torch.randn(100, 100).cuda()
            y = torch.randn(100, 100).cuda()
            z = torch.mm(x, y)
            print("‚úÖ Opera√ß√£o GPU: OK")
        except Exception as e:
            print(f"‚ùå Erro GPU: {e}")

    # Teste Transformers
    try:
        from transformers import AutoTokenizer
        print("‚úÖ Transformers: OK")
    except ImportError:
        print("‚ùå Transformers: N√£o instalado")

    # Teste Orquestrador
    try:
        from glpi_dashboard.backend.scripts.glpi_ai_orchestrator_minimal import GLPIAIOrchestrator
        orchestrator = GLPIAIOrchestrator()
        status = orchestrator.get_status()
        print(f"‚úÖ Orquestrador: {status}")
    except ImportError as e:
        print(f"‚ùå Orquestrador: {e}")

    print("\nüéâ Teste conclu√≠do!")

if __name__ == "__main__":
    test_environment()
'''

        test_path = self.scripts_dir / "test_gpu_minimal.py"
        test_path.write_text(test_code, encoding='utf-8')
        logger.info(f"üß™ Teste criado: {test_path}")
        return test_path

    def generate_report(self, success_packages, gpu_available, pytorch_working):
        """Gera relat√≥rio final"""
        logger.info("üìä Gerando relat√≥rio...")

        report = f"""# Relat√≥rio - Configura√ß√£o GPU M√≠nima
## GLPI Dashboard - Integra√ß√£o IA

**Data**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
**Modo**: Instala√ß√£o M√≠nima (Otimizada para Espa√ßo)

## üéØ Resumo da Configura√ß√£o

- **GPU Detectada**: {'‚úÖ Sim' if gpu_available else '‚ùå N√£o'}
- **PyTorch Funcionando**: {'‚úÖ Sim' if pytorch_working else '‚ùå N√£o'}
- **Pacotes Instalados**: {success_packages}/{len(self.essential_packages)}

## üì¶ Pacotes Essenciais

{''.join([f"- {pkg.split()[0]}: ‚úÖ\n" for pkg in self.essential_packages])}

## ü§ñ Componentes Criados

- **Orquestrador**: `glpi_ai_orchestrator_minimal.py` ‚úÖ
- **Teste**: `test_gpu_minimal.py` ‚úÖ

## üöÄ Pr√≥ximos Passos

### 1. Teste da Configura√ß√£o
```bash
python scripts/test_gpu_minimal.py
```

### 2. Uso do Orquestrador
```python
from glpi_dashboard.backend.scripts.glpi_ai_orchestrator_minimal import GLPIAIOrchestrator

        orchestrator = GLPIAIOrchestrator()
status = orchestrator.get_status()
print(status)
```

### 3. An√°lise de Logs GLPI
```python
log_text = "Error connecting to database"
analysis = orchestrator.analyze_glpi_log(log_text)
print(analysis)
```

## üí° Modelos Compactos Recomendados

{''.join([f"- {model} (Compacto)\n" for model in self.compact_models])}

## ‚ö†Ô∏è Limita√ß√µes Atuais

- **Espa√ßo em Disco**: Configura√ß√£o m√≠nima devido a limita√ß√µes
- **TensorFlow**: N√£o instalado (conflitos de depend√™ncia)
- **LangChain**: N√£o instalado (problemas de espa√ßo)
- **Gradio/Streamlit**: N√£o instalado (sem espa√ßo)

## üîß Otimiza√ß√µes Implementadas

- Cache configurado no Drive B: (1,847GB dispon√≠veis)
- Apenas pacotes essenciais instalados
- Modelos compactos recomendados
- Orquestrador otimizado para recursos limitados

## üìà Benef√≠cios Esperados

- **Performance**: GPU NVIDIA RTX A4000 16GB
- **An√°lise**: Processamento b√°sico de logs GLPI
- **Embeddings**: Gera√ß√£o de representa√ß√µes de texto
- **Escalabilidade**: Base para expans√£o futura
"""

        report_path = self.project_root / "relatorio_gpu_minimal.md"
        report_path.write_text(report, encoding='utf-8')
        logger.info(f"üìä Relat√≥rio salvo: {report_path}")
        return report_path

    def run_setup(self):
        """Executa configura√ß√£o completa"""
        logger.info("üöÄ Iniciando configura√ß√£o GPU m√≠nima...")

        # 1. Verificar GPU
        gpu_available = self.check_gpu()

        # 2. Instalar pacotes essenciais
        success_packages = 0
        if self.install_essentials():
            success_packages = len([pkg for pkg in self.essential_packages])

        # 3. Testar PyTorch
        pytorch_working = self.test_pytorch_gpu()

        # 4. Criar componentes
        self.create_minimal_orchestrator()
        self.create_test_script()

        # 5. Gerar relat√≥rio
        report_path = self.generate_report(success_packages, gpu_available, pytorch_working)

        # Resumo final
        logger.info("\n" + "=" * 60)
        logger.info("üéâ CONFIGURA√á√ÉO GPU M√çNIMA CONCLU√çDA!")
        logger.info("=" * 60)
        logger.info(f"üìä Relat√≥rio: {report_path}")
        logger.info(f"üß™ Teste: python scripts/test_gpu_minimal.py")
        logger.info(f"ü§ñ Orquestrador: scripts/glpi_ai_orchestrator_minimal.py")

        if pytorch_working:
            logger.info("‚úÖ GPU + PyTorch funcionando!")
        else:
            logger.warning("‚ö†Ô∏è Problemas com GPU/PyTorch")

        return True

if __name__ == "__main__":
    print("üîß Configura√ß√£o GPU M√≠nima - GLPI Dashboard")
    print("=" * 50)

    setup = GPUMinimalSetup()
    setup.run_setup()
