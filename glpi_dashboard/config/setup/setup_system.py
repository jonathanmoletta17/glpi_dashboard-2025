#!/usr/bin/env python3
"""
Script de configura√ß√£o do sistema GLPI Dashboard AI Agent
"""

import os
import yaml
import torch
from pathlib import Path
import json
from datetime import datetime

def create_directories():
    """Cria diret√≥rios necess√°rios do sistema"""
    directories = [
        "models",
        "logs", 
        "cache",
        "temp",
        "reports",
        "workflows",
        "templates",
        "data"
    ]
    
    print("üìÅ Criando diret√≥rios do sistema...")
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"  ‚úÖ {directory}/")
    
    return True

def check_system_requirements():
    """Verifica requisitos do sistema"""
    print("üîç Verificando requisitos do sistema...")
    
    # Verificar Python
    import sys
    python_version = sys.version_info
    print(f"  Python: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    # Verificar PyTorch
    print(f"  PyTorch: {torch.__version__}")
    
    # Verificar CUDA
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"  GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        print(f"  CUDA: {torch.version.cuda}")
    else:
        print("  GPU: N√£o dispon√≠vel")
    
    # Verificar depend√™ncias
    try:
        import transformers
        print(f"  Transformers: {transformers.__version__}")
    except ImportError:
        print("  ‚ùå Transformers n√£o instalado")
        return False
    
    try:
        import accelerate
        print(f"  Accelerate: {accelerate.__version__}")
    except ImportError:
        print("  ‚ùå Accelerate n√£o instalado")
        return False
    
    return True

def load_working_models():
    """Carrega lista de modelos funcionais"""
    models_file = Path("working_models.txt")
    if models_file.exists():
        with open(models_file, 'r') as f:
            models = [line.strip() for line in f if line.strip()]
        return models
    return []

def create_system_config():
    """Cria configura√ß√£o personalizada do sistema"""
    print("‚öôÔ∏è Criando configura√ß√£o do sistema...")
    
    # Verificar modelos dispon√≠veis
    working_models = load_working_models()
    
    # Configura√ß√£o b√°sica
    config = {
        "system": {
            "name": "GLPI Dashboard AI Agent System",
            "version": "1.0.0",
            "environment": "development",
            "debug_mode": True,
            "log_level": "INFO",
            "created_at": datetime.now().isoformat()
        },
        "hardware": {
            "gpu": {
                "available": torch.cuda.is_available(),
                "device_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
                "memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0,
                "cuda_version": torch.version.cuda if torch.cuda.is_available() else None
            },
            "pytorch_version": torch.__version__
        },
        "models": {
            "available": working_models,
            "primary": working_models[0] if working_models else None,
            "fallback": "microsoft/DialoGPT-small"
        },
        "features": {
            "code_analysis": True,
            "glpi_integration": True,
            "dashboard_generation": True,
            "ai_assistance": True
        }
    }
    
    # Salvar configura√ß√£o
    config_file = Path("../system.yaml")
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"  ‚úÖ Configura√ß√£o salva em: {config_file}")
    return config

def test_system_integration():
    """Testa integra√ß√£o b√°sica do sistema"""
    print("üß™ Testando integra√ß√£o do sistema...")
    
    try:
        # Teste b√°sico de importa√ß√µes
        from transformers import AutoTokenizer
        print("  ‚úÖ Transformers funcionando")
        
        # Teste de GPU se dispon√≠vel
        if torch.cuda.is_available():
            test_tensor = torch.randn(10, 10).cuda()
            result = test_tensor @ test_tensor.T
            print("  ‚úÖ GPU funcionando")
        
        # Teste de modelo simples se dispon√≠vel
        working_models = load_working_models()
        if working_models:
            model_name = working_models[0]
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            print(f"  ‚úÖ Modelo {model_name} acess√≠vel")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erro na integra√ß√£o: {e}")
        return False

def generate_system_report():
    """Gera relat√≥rio do sistema"""
    print("üìä Gerando relat√≥rio do sistema...")
    
    working_models = load_working_models()
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "system_status": "operational",
        "components": {
            "python": "‚úÖ Funcionando",
            "pytorch": "‚úÖ Funcionando",
            "cuda": "‚úÖ Funcionando" if torch.cuda.is_available() else "‚ùå N√£o dispon√≠vel",
            "transformers": "‚úÖ Funcionando",
            "models": f"‚úÖ {len(working_models)} modelos dispon√≠veis"
        },
        "models_available": working_models,
        "next_steps": [
            "Configurar integra√ß√£o com GLPI",
            "Testar gera√ß√£o de dashboards",
            "Validar an√°lise de c√≥digo",
            "Implementar workflows automatizados"
        ]
    }
    
    # Salvar relat√≥rio
    report_file = Path("reports/system_setup_report.json")
    report_file.parent.mkdir(exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"  ‚úÖ Relat√≥rio salvo em: {report_file}")
    return report

def main():
    print("üöÄ GLPI Dashboard AI Agent System - Configura√ß√£o")
    print("=" * 60)
    
    # Executar configura√ß√£o
    steps = [
        ("Criar diret√≥rios", create_directories),
        ("Verificar requisitos", check_system_requirements),
        ("Criar configura√ß√£o", create_system_config),
        ("Testar integra√ß√£o", test_system_integration),
        ("Gerar relat√≥rio", generate_system_report)
    ]
    
    results = {}
    
    for step_name, step_func in steps:
        print(f"\n{step_name}...")
        try:
            result = step_func()
            results[step_name] = result
            if result:
                print(f"‚úÖ {step_name} conclu√≠do")
            else:
                print(f"‚ö†Ô∏è {step_name} com problemas")
        except Exception as e:
            print(f"‚ùå Erro em {step_name}: {e}")
            results[step_name] = False
    
    # Resumo final
    print("\n" + "=" * 60)
    print("RESUMO DA CONFIGURA√á√ÉO")
    print("=" * 60)
    
    success_count = sum(1 for result in results.values() if result)
    total_count = len(results)
    
    print(f"‚úÖ Etapas conclu√≠das: {success_count}/{total_count}")
    
    if success_count == total_count:
        print("üéâ Sistema configurado com sucesso!")
        print("\nüìã Pr√≥ximos passos:")
        print("  1. Aguardar download completo dos modelos")
        print("  2. Testar integra√ß√£o com GLPI")
        print("  3. Validar gera√ß√£o de dashboards")
    else:
        print("‚ö†Ô∏è Configura√ß√£o parcial. Verifique os erros acima.")

if __name__ == "__main__":
    main()